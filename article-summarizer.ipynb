{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Deb St. Cyr\n",
    "Link to GitHub: https://github.com/14dstcyr/D7-Applying-Course-Skills/tree/main\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the requirements are installed\n",
      "Package                   Version\n",
      "------------------------- --------------\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.6.2.post1\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "blis                      1.0.1\n",
      "catalogue                 2.0.10\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "click                     8.1.7\n",
      "cloudpathlib              0.20.0\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "confection                0.1.5\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "cymem                     2.0.10\n",
      "debugpy                   1.8.9\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "en_core_web_sm            3.8.0\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.21.1\n",
      "fonttools                 4.55.0\n",
      "fqdn                      1.5.1\n",
      "h11                       0.14.0\n",
      "html5lib                  1.1\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.0\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.30.0\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.6\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "kiwisolver                1.4.7\n",
      "langcodes                 3.5.0\n",
      "language_data             1.3.0\n",
      "marisa-trie               1.2.1\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.9.3\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.0.2\n",
      "murmurhash                1.0.11\n",
      "nbclient                  0.10.1\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.9.1\n",
      "notebook                  7.2.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.0.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    11.0.0\n",
      "pip                       24.3.1\n",
      "platformdirs              4.3.6\n",
      "preshed                   3.0.9\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.48\n",
      "psutil                    6.1.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "pydantic                  2.10.2\n",
      "pydantic_core             2.27.1\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pywin32                   308\n",
      "pywinpty                  2.0.14\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.4\n",
      "rpds-py                   0.22.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.6.0\n",
      "shellingham               1.5.4\n",
      "six                       1.16.0\n",
      "smart-open                7.0.5\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "spacy                     3.8.2\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.5\n",
      "spacytextblob             5.0.0\n",
      "srsly                     2.4.8\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "textblob                  0.18.0.post0\n",
      "thinc                     8.3.2\n",
      "tinycss2                  1.4.0\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "typer                     0.14.0\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "typing_extensions         4.12.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wasabi                    1.1.3\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.4.1\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.13\n",
      "wrapt                     1.17.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "print('All the requirements are installed')\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install  requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics saved tosong_lyrics\\Band  On The Run.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "\n",
    "result = json.loads(requests.get('https://api.lyrics.ovh/v1/Paul McCartney/Band On The Run').text)\n",
    "\n",
    "# Create a folder and file path\n",
    "folder_name = 'song_lyrics'\n",
    "file_name = 'Band  On The Run.json'\n",
    "\n",
    "# If folder does not exist, create one\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Create path to file\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "# Write the JSON file result\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"Lyrics saved to{file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Score: 0.18055555555555558\n",
      "The score is closer to 1. The lyrics have a more positive sentiment.\n",
      "Number of Sentences: 14\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add spacytextblob to pipeline\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "\n",
    "# Get the lyrics from the API and process them\n",
    "response = requests.get('https://api.lyrics.ovh/v1/Paul McCartney/Band On The Run')\n",
    "result = json.loads(response.text)\n",
    "\n",
    "# Extract lyrics text from the API response\n",
    "lyrics = result.get(\"lyrics\", \"\")  # Fallback to an empty string if 'lyrics' key is missing\n",
    "\n",
    "if lyrics:\n",
    "    # Process the lyrics with spaCy\n",
    "    doc = nlp(lyrics)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    polarity_score = doc._.blob.polarity\n",
    "\n",
    "    # Print polarity score with an appropriate label\n",
    "    print(\"Polarity Score:\", polarity_score)\n",
    "\n",
    "    if polarity_score > 0:\n",
    "        print(\"The score is closer to 1. The lyrics have a more positive sentiment.\")\n",
    "    elif polarity_score < 0:\n",
    "        print(\"The score is closer to -1. The lyrics have a more negative sentiment.\")\n",
    "    else:\n",
    "        print(\"The score is 0. The lyrics have a neutral sentiment.\")\n",
    "\n",
    "    # Count the number of sentences in the lyrics\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    print(\"Number of Sentences:\", num_sentences)\n",
    "\n",
    "else:\n",
    "    print(\"Could not retrieve lyrics from the API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Common Tokens in the Lyrics:\n",
      "The token 'run' has a frequency of 20\n",
      "The token 'band' has a frequency of 19\n",
      "The token 'searching' has a frequency of 3\n",
      "The token 'inside' has a frequency of 2\n",
      "The token 'seeing' has a frequency of 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Fetch lyrics from the API\n",
    "response = requests.get('https://api.lyrics.ovh/v1/Paul McCartney/Band On The Run')\n",
    "result = response.json()  # Parse the JSON response\n",
    "lyrics_text = result.get(\"lyrics\", \"\")  # Get the lyrics from the JSON response\n",
    "\n",
    "# Ensure we have the lyrics text\n",
    "if not lyrics_text:\n",
    "    print(\"Failed to retrieve lyrics from the API.\")\n",
    "else:\n",
    "    # Load spaCy pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the lyrics text\n",
    "    article = nlp(lyrics_text)\n",
    "\n",
    "    # Filter out punctuation, whitespace, and stopwords\n",
    "    tokens = [\n",
    "        token.text.lower() for token in article \n",
    "        if not token.is_punct and not token.is_stop and not token.is_space\n",
    "    ]\n",
    "\n",
    "    # Count the 5 most common tokens\n",
    "    token_counts = Counter(tokens)\n",
    "    common_tokens = token_counts.most_common(5)\n",
    "\n",
    "    # Print the 5 most common tokens and their frequencies\n",
    "    print(\"5 Most Common Tokens in the Lyrics:\")\n",
    "    for token, freq in common_tokens:\n",
    "        print(f\"The token '{token}' has a frequency of {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Common Lemmas in the Article:\n",
      "The lemma 'run' has a frequency of 20\n",
      "The lemma 'band' has a frequency of 19\n",
      "The lemma 'search' has a frequency of 4\n",
      "The lemma 'inside' has a frequency of 2\n",
      "The lemma 'see' has a frequency of 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Grab lyrics from the API\n",
    "response = requests.get('https://api.lyrics.ovh/v1/Paul McCartney/Band On The Run')\n",
    "result = response.json()  # Parse the JSON response\n",
    "lyrics_text = result.get(\"lyrics\", \"\")  # Get the lyrics from the JSON response\n",
    "\n",
    "# Make sure the text lyrics are there\n",
    "if not lyrics_text:\n",
    "    print(\"Failed to retrieve lyrics from API\")\n",
    "else:\n",
    "    # Load the spaCy pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the article text\n",
    "    doc = nlp(lyrics_text)\n",
    "    \n",
    "    # Extract lemmas, excluding stopwords, punctuation, and whitespace\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "    \n",
    "    # Count the lemmas\n",
    "    lemma_counts = Counter(lemmas)\n",
    "    \n",
    "    # Get the 5 most common lemmas\n",
    "    common_lemmas = lemma_counts.most_common(5)\n",
    "    \n",
    "    # Print the 5 most common lemmas with their frequencies\n",
    "    print(\"5 Most Common Lemmas in the Article:\")\n",
    "    for lemma, freq in common_lemmas:\n",
    "        print(f\"The lemma '{lemma}' has a frequency of {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
